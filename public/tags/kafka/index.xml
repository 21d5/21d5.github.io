<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kafka on 阳明的博客</title>
    <link>https://www.qikqiak.com/tags/kafka/</link>
    <description>Recent content in Kafka on 阳明的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 01 Nov 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.qikqiak.com/tags/kafka/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>在 Kubernetes 上运行高可用的 Kafka 集群</title>
      <link>https://www.qikqiak.com/post/deploy-kafka-ha-on-k8s/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.qikqiak.com/post/deploy-kafka-ha-on-k8s/</guid>
      <description>&lt;p&gt;Apache Kafka 是目前最流行的分布式消息发布订阅系统，虽然 Kafka 非常强大，但它同样复杂，需要一个高可用的强大平台来运行。在微服务盛行，大多数公司都采用分布式计算的今天，将 Kafka 作为核心的消息系统使用还是非常有优势的。&lt;/p&gt;

&lt;p&gt;如果你在 Kubernetes 集群中运行你的微服务，那么在 Kubernetes 中运行 Kafka 集群也是很有意义的，这样可以利用其内置的弹性和高可用，我们可以使用内置的 Kubernetes 服务发现轻松地与集群内的 Kafka Pods 进行交互。&lt;/p&gt;

&lt;p&gt;下面我们将来介绍下如何在 Kubernetes 上构建分布式的 Kafka 集群，这里我们将使用 Helm Chart 和 StatefulSet 来进行部署，当然如果想要动态生成持久化数据卷，还需要提前配置一个 StorageClass 资源，比如基于 Ceph RBD 的，如果你集群中没有配置动态卷，则需要提前创建3个未绑定的 PV 用于数据持久化。&lt;/p&gt;

&lt;p&gt;当前基于 Helm 官方仓库的 &lt;code&gt;chartincubator/kafka&lt;/code&gt; 在 Kubernetes 上部署的 Kafka，使用的镜像是 &lt;code&gt;confluentinc/cp-kafka:5.0.1&lt;/code&gt;，即部署的是Confluent 公司提供的 Kafka 版本，Confluent Platform Kafka(简称CP Kafka)提供了一些 Apache Kafka 没有的高级特性，例如跨数据中心备份、Schema 注册中心以及集群监控工具等。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>在 Kubernetes 集群上部署 Kafka</title>
      <link>https://www.qikqiak.com/post/install-kafka-in-kubernetes/</link>
      <pubDate>Tue, 17 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.qikqiak.com/post/install-kafka-in-kubernetes/</guid>
      <description>&lt;p&gt;最近在测试日志采集的时候，发现日志数据量稍微大一点，Elasticsearch 就有点抗不住了，对于 ES 的优化可能不是一朝一夕能够完成的，所以打算加一个中间层，将日志输出到 Kafka，然后通过 Logstash 从 Kafka 里面去消费日志存入 Elasticsearch。在测试环境现在并没有一套 Kafka 集群，所以我们来先在测试环境搭建一套 Kafka 集群。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>